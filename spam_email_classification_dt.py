# -*- coding: utf-8 -*-
"""Spam Email Classification-DT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15R_I_sA6X-lJ71HDhz9y-fDeFC9IDQ3-

###The objective of this dataset would typically be to train a machine learning model to predict whether an email is spam or not
Rows: Each row represents an individual email.
Columns: The columns likely represent features extracted from the email bodies. There are 3000 columns containing words that are part of the email bodies.

##Importing necessary Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

"""###Loading the dataset"""

df= pd.read_csv("emails.csv")

"""##Analyzing the dataset"""

df

# Checking if there is any null data
df.isnull().count()

df.info()

# Plotting null data in the heatmap
import seaborn as sns
sns.heatmap(df.isnull())

#Checking for unique data
df.nunique()

df.columns

df.shape

df.describe()

df= df.iloc[:,1:]

df

x = df.iloc[:, :3000].values
x

y = df.iloc[:, -1].values
y

"""##Splitting the data for Train and Test"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state= 1)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
# Initialize the decision tree classifier
dt_classifier = DecisionTreeClassifier(max_depth=None, min_samples_leaf=1, criterion='gini')

# Fit the classifier to the training data
dt_classifier.fit(x_train, y_train)

# Predict on the testing data
y_pred = dt_classifier.predict(x_test)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Precision
precision = precision_score(y_test, y_pred, average='weighted')
print("Precision:", precision)

# Recall
recall = recall_score(y_test, y_pred, average='weighted')
print("Recall:", recall)

# F1-score
f1 = f1_score(y_test, y_pred, average='weighted')
print("F1-score:", f1)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""##1) Accuracy: 94.01% of the predictions made by the model were correct.
##2) Precision: For class 0 (non-spam), 94.02% of the emails predicted as non-spam were actually non-spam. For class 1 (spam), 90% of the emails predicted as spam were actually spam.
##3) Recall: For class 0, 94.01% of the actual non-spam emails were correctly predicted as non-spam. For class 1, 91% of the actual spam emails were correctly predicted as spam.
##4) F1-score: This is the harmonic mean of precision and recall. It gives a balance between precision and recall. The F1-score for class 0 is 96% and for class 1 is 90%.
##Looking at the classification report, it seems that the model performs well in distinguishing between spam and non-spam emails, with high precision, recall, and F1-score for both classes. However, it's essential to consider the specific requirements of the application and potentially investigate any misclassifications or areas for improvement. Overall, it indicates a robust performance of the classification model on the given dataset.
"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Generate predictions using the decision tree classifier
predictions = dt_classifier.predict(x_test)

# Access the classes_ attribute to get the class names
class_names = dt_classifier.classes_.astype(str)

# Plot the decision tree
plt.figure(figsize=(20, 10))
plot_tree(dt_classifier, class_names=class_names, filled=True)
plt.show()

import seaborn as sns
from sklearn.metrics import confusion_matrix

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

from sklearn.model_selection import cross_val_score

# Perform cross-validation
scores = cross_val_score(dt_classifier, x, y, cv=5, scoring='accuracy')

# Print cross-validation scores
print("Cross-validation scores:", scores)

# Calculate and print mean cross-validation score
mean_score = scores.mean()
print("Mean cross-validation score:", mean_score)

"""###These scores represent the performance of the model across different folds of the cross-validation process. Each score corresponds to the accuracy or performance metric (such as precision, recall, or F1-score) achieved on a particular fold of the data.

###The mean cross-validation score, calculated by averaging these individual scores, is 0.9102843420327232. This mean score provides an overall measure of how well the model generalizes to unseen data. In this case, it suggests that, on average, the model achieves an accuracy or performance metric score of approximately 91.03% across the different folds of the cross-validation process.

###Cross-validation is a technique used to assess the performance and generalization ability of a predictive model by splitting the dataset into multiple subsets, training the model on a portion of the data, and evaluating it on the remaining unseen data. The cross-validation scores help you understand how well your model performs on different parts of the dataset and provide insights into its stability and robustness.
"""